{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF Text extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from settings import *\n",
    "import fitz\n",
    "import gradio as gr\n",
    "\n",
    "# os.environ[\"GRADIO_ANALYTICS_ENABLED\"] = \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT BEING USED\n",
    "\n",
    "\n",
    "# def get_db_connection():\n",
    "#     try:\n",
    "#         print(\"Attempting to connect to database...\")\n",
    "#         print(\"Host: mysql-samadhan-parakhc4.b.aivencloud.com, Port: 12945\")\n",
    "#         connection = pymysql.connect(\n",
    "#             charset=\"utf8mb4\",\n",
    "#             connect_timeout=10,\n",
    "#             cursorclass=pymysql.cursors.DictCursor,\n",
    "#             db=\"doctors_database\",\n",
    "#             host=\"mysql-samadhan-parakhc4.b.aivencloud.com\",\n",
    "#             password=\"AVNS_iy-fKKvQNt-u1-gHDcW\",\n",
    "#             read_timeout=10,\n",
    "#             port=12945,\n",
    "#             user=\"avnadmin\",\n",
    "#             write_timeout=10,\n",
    "#         )\n",
    "#         print(\"Database connection successful.\")\n",
    "#         return connection\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error connecting to the database: {e}\")\n",
    "#         raise\n",
    "    \n",
    "# conn = sqlite3.connect('doctors.db')\n",
    "# cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAEMATOLOGY\n",
      "COMPLETE BLOOD COUNT (CBC)\n",
      "TEST\n",
      "VALUE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing report text extraction functionality\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(document)):\n",
    "        page = document[page_num]\n",
    "        text += page.get_text()\n",
    "    document.close()\n",
    "    return text.strip()\n",
    "\n",
    "pdf_text = extract_text_from_pdf(\"cbc-report-format.pdf\")\n",
    "print(pdf_text[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_specific_intent_restriction(generated_text):\n",
    "    message = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant skilled in verifying whether provided content is within restricted boundary\"},\n",
    "            {\"role\": \"user\", \"content\": f\"A chat Agent has generated the following summary of a PDF file. Please verify if the summary is of a Medical Health report. Reply with Yes or No only.:\\n\\n{generated_text}\"}\n",
    "        ]\n",
    "    response = monster_client.chat.completions.create(\n",
    "        model=monster_ai_model_name[llm_name],\n",
    "        messages=message,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens,\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    binary_classifier = response.choices[0].message.content\n",
    "\n",
    "    if binary_classifier == \"Yes\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_llm(user_input, file=None):\n",
    "    try:\n",
    "        # If a file is uploaded, extract text\n",
    "        pdf_text = \"\"\n",
    "        if file:\n",
    "            pdf_text = extract_text_from_pdf(file.name)\n",
    "            if not pdf_text.strip():\n",
    "                return \"The uploaded PDF appears to be empty. Please try with a valid medical report.\"\n",
    "\n",
    "        # Create context based on input and optional PDF content\n",
    "        context = f\"Medical report content: {pdf_text}\" if file else \"\"\n",
    "        message = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant skilled in answering questions about medical reports.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{context}\\n\\n{user_input}\"}\n",
    "        ]\n",
    "\n",
    "        # Generate response using Monster API\n",
    "        response = monster_client.chat.completions.create(\n",
    "            model=monster_ai_model_name[llm_name],\n",
    "            messages=message,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens,\n",
    "            stream=False\n",
    "        )\n",
    "\n",
    "        # Extract the chat response\n",
    "        generated_text = response.choices[0].message.content\n",
    "        return generated_text\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error occurred: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from qdrant_client import models, QdrantClient\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # Initialize the encoder and Qdrant client\n",
    "# encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# client = QdrantClient(\":memory:\")  # For in-memory use; replace with cluster host info for persistent storage\n",
    "\n",
    "\n",
    "# collection_name = \"medical_reports\"\n",
    "# client.create_collection(\n",
    "#     collection_name=collection_name,\n",
    "#     vectors_config=models.VectorParams(\n",
    "#         size=encoder.get_sentence_embedding_dimension(),  # Vector size is defined by the encoder\n",
    "#         distance=models.Distance.COSINE,  # Use cosine similarity\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# def chunk_text(text, chunk_size=500):\n",
    "#     return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# chunks = chunk_text(pdf_text)\n",
    "\n",
    "# client.upload_points(\n",
    "#     collection_name=collection_name,\n",
    "#     points=[\n",
    "#         models.PointStruct(\n",
    "#             id=idx,\n",
    "#             vector=encoder.encode(chunk).tolist(),\n",
    "#             payload={\"chunk\": chunk}\n",
    "#         )\n",
    "#         for idx, chunk in enumerate(chunks)\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# print(f\"Stored {len(chunks)} chunks in Qdrant.\")\n",
    "\n",
    "\n",
    "# def query_relevant_chunks(query_text, top_k=5):\n",
    "#     query_vector = encoder.encode(query_text).tolist()\n",
    "#     hits = client.query_points(\n",
    "#         collection_name=collection_name,\n",
    "#         query=query_vector,\n",
    "#         limit=top_k,\n",
    "#     ).points\n",
    "#     return [hit.payload[\"chunk\"] for hit in hits]\n",
    "\n",
    "# query = \"summarize the medical findings\"\n",
    "# relevant_chunks = query_relevant_chunks(query)\n",
    "\n",
    "# print(\"Relevant chunks for summarization:\")\n",
    "# for chunk in relevant_chunks:\n",
    "#     print(chunk[:200], \"...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# from settings import *\n",
    "# import os\n",
    "\n",
    "# generation_model_name: str\n",
    "# temperature: float = 0.9\n",
    "# top_p = 0.9\n",
    "# max_tokens: int = 2048\n",
    "# stream: bool = True\n",
    "# llm_name: str = \"Meta-Llama\"\n",
    "\n",
    "# monster_client = OpenAI(\n",
    "#     base_url=\"https://llm.monsterapi.ai/v1/\",\n",
    "#     api_key=str(MONSTER_API_KEY)\n",
    "# )\n",
    "\n",
    "# monster_ai_model_name = {\n",
    "#     \"Google-Gemma\": \"google/gemma-2-9b-it\",\n",
    "#     \"Mistral\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "#     \"Microsoft-Phi\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "#     \"Meta-Llama\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "# }\n",
    "\n",
    "# # Context and Summarization Prompt\n",
    "# message = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are an AI assistant skilled in summarizing complex medical reports.\"},\n",
    "#     {\"role\": \"user\", \"content\": f\"Summarize the following medical report in simple terms. Focus on the key findings, diagnoses, and recommendations. Report text:\\n\\n{pdf_text}\"}\n",
    "# ]\n",
    "\n",
    "# # Generate Summary\n",
    "# response = monster_client.chat.completions.create(\n",
    "#     model=monster_ai_model_name[llm_name],\n",
    "#     messages=message,\n",
    "#     temperature=temperature,\n",
    "#     top_p=top_p,\n",
    "#     max_tokens=max_tokens,\n",
    "#     stream=stream\n",
    "# )\n",
    "\n",
    "# # Collect Generated Text\n",
    "# generated_text = \"\"\n",
    "# for chunk in response:\n",
    "#     if chunk.choices[0].delta.content is not None:\n",
    "#         generated_text += chunk.choices[0].delta.content\n",
    "\n",
    "# print(\"Generated Summary:\")\n",
    "# print(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYMPTOM_SPECIALTY_MAP = {\n",
    "    \"abdominal pain\": \"Gastroenterologist\",\n",
    "    \"pregnancy\": \"Gynecologist\",\n",
    "    \"fever\": \"General Physician\",\n",
    "    \"chest pain\": \"Cardiologist\",\n",
    "    \"skin rash\": \"Dermatologist\",\n",
    "    \"depression\" : \"Psychiatrist\",\n",
    "    \"headache\" : \"Neurologist\",\n",
    "}\n",
    "\n",
    "def get_doctors_by_specialty(specialty):\n",
    "    try:\n",
    "        connection = get_db_connection()\n",
    "        with connection.cursor() as cursor:\n",
    "\n",
    "            query = \"\"\"\n",
    "                SELECT id, name, specialization, latitude, longitude, address, contact \n",
    "                FROM doctors \n",
    "                WHERE LOWER(specialization) = LOWER(%s)\n",
    "            \"\"\"\n",
    "\n",
    "            cleaned_specialty = specialty.strip()\n",
    "            print(f\"Executing query for: '{cleaned_specialty}'\")  # Debugging\n",
    "\n",
    "            cursor.execute(query, (cleaned_specialty,))\n",
    "            doctors = cursor.fetchall()\n",
    "            print(\"Query result:\", doctors)  # Debugging output\n",
    "        connection.close()\n",
    "\n",
    "        return doctors if doctors else []\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching doctors: {e}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_doctors_by_specialty(specialty):\n",
    "#     try:\n",
    "#         connection = get_db_connection()\n",
    "#         with connection.cursor() as cursor:\n",
    "#             query = \"\"\"\n",
    "#                 SELECT id, name, specialization, latitude, longitude, address, contact \n",
    "#                 FROM doctors \n",
    "#                 WHERE LOWER(specialization) = LOWER(%s)\n",
    "#             \"\"\"\n",
    "\n",
    "#             cursor.execute(query, (specialty,))\n",
    "#             doctors = cursor.fetchall()\n",
    "#             print(\"Query result:\", doctors)  # Debugging output\n",
    "#         connection.close()\n",
    "\n",
    "#         return doctors if doctors else []\n",
    "#     except Exception as e:\n",
    "#         return f\"Error fetching doctors: {e}\"\n",
    "\n",
    "def recommend_doctors(symptoms):\n",
    "\n",
    "    specialty = None\n",
    "    for symptom, spec in SYMPTOM_SPECIALTY_MAP.items():\n",
    "        if symptom in symptoms.lower():\n",
    "            specialty = spec\n",
    "            break\n",
    "    \n",
    "    print(\"Inferred specialty:\", specialty)  # Debugging\n",
    "\n",
    "    if not specialty:\n",
    "        return \"Sorry, we couldn't infer a relevant specialty for your symptoms. Please provide more details.\"\n",
    "    specialty = specialty.strip()\n",
    "\n",
    "\n",
    "    doctors = get_doctors_by_specialty(specialty)\n",
    "    print(\"Doctors fetched:\", doctors)  # Debugging output\n",
    "\n",
    "    if not doctors:\n",
    "        return f\"No doctors found for the specialty: {specialty}.\"\n",
    "\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"Dr. {doc['name']} ({doc['specialization']})\\nAddress: {doc['address']}\\nContact: {doc['contact']}\" \n",
    "        for doc in doctors\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://a177c873e7cda69cb1.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a177c873e7cda69cb1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# PDF Text Extraction \n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(document)):\n",
    "        page = document[page_num]\n",
    "        text += page.get_text()\n",
    "    document.close()\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "generation_model_name: str\n",
    "temperature: float = 0.9\n",
    "top_p = 0.9\n",
    "max_tokens: int = 2048\n",
    "stream: bool = False  \n",
    "llm_name: str = \"Meta-Llama\"\n",
    "\n",
    "monster_client = OpenAI(\n",
    "    base_url=\"https://llm.monsterapi.ai/v1/\",\n",
    "    api_key=str(MONSTER_API_KEY)\n",
    ")\n",
    "\n",
    "monster_ai_model_name = {\n",
    "    \"Google-Gemma\": \"google/gemma-2-9b-it\",\n",
    "    \"Mistral\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"Microsoft-Phi\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    \"Meta-Llama\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "}\n",
    "\n",
    "# Summarization Function\n",
    "def summarize_pdf(file):\n",
    "    try:\n",
    "        if not file.name.endswith(\".pdf\"):\n",
    "            return \"Invalid file format. Please upload a PDF.\"\n",
    "\n",
    "\n",
    "        pdf_text = extract_text_from_pdf(file.name)\n",
    "        if not pdf_text.strip():\n",
    "            return \"The uploaded PDF appears to be empty. Please try with a valid medical report.\"\n",
    "\n",
    "\n",
    "        message = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant skilled in summarizing complex medical reports. Strict instruction: Only summarise a report if it is a medical report. Otherwise, reply with 'Attached PDF is not a medical report'\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Summarize the following medical report in simple terms. Focus on the key findings, diagnoses, and recommendations. Report text:\\n\\n{pdf_text}\"}\n",
    "        ]\n",
    "\n",
    "\n",
    "        response = monster_client.chat.completions.create(\n",
    "            model=monster_ai_model_name[llm_name],\n",
    "            messages=message,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_tokens=max_tokens,\n",
    "            stream=False\n",
    "        )\n",
    "\n",
    "        # Extract the summary from the response\n",
    "        generated_text = response.choices[0].message.content\n",
    "\n",
    "        # validation \n",
    "        return generated_text\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error occurred: {e}\"\n",
    "\n",
    "\n",
    "\n",
    "# Gradio Interface\n",
    "# interface = gr.Interface(\n",
    "#     fn=chat_with_llm,\n",
    "#     inputs=[\n",
    "#         gr.Textbox(lines=2, placeholder=\"Type your question here...\", label=\"Ask a Question\"),\n",
    "#         gr.File(label=\"Optional: Upload your Medical Report (PDF)\")\n",
    "#     ],\n",
    "#     outputs=gr.Textbox(label=\"AI's Response\"),\n",
    "#     title=\"Medical Report Chat Assistant\",\n",
    "#     description=(\n",
    "#         \"Ask questions about your medical report or any medical context. \"\n",
    "#         \"Optionally, upload a PDF report for more tailored answers.\"\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# Gradio UI\n",
    "with gr.Blocks() as interface:\n",
    "    gr.Markdown(\"# 🥼 Samadhan Conversational HealthBot 1.0\")\n",
    "    gr.Markdown(\"Upload a medical report or describe your symptoms to get recommendations.\")\n",
    "\n",
    "    with gr.Tab(\"🩻 Summarize Report\"):\n",
    "        file_input = gr.File(label=\"Upload your Medical Report (PDF)\")\n",
    "        summary_output = gr.Textbox(label=\"Summary\")\n",
    "        summarize_btn = gr.Button(\"Summarize\")\n",
    "        summarize_btn.click(summarize_pdf, inputs=file_input, outputs=summary_output)\n",
    "\n",
    "    with gr.Tab(\"💬 Chat with AI\"):\n",
    "        chat_input = gr.Textbox(lines=2, placeholder=\"Ask a question about your medical report...\", label=\"Your Question\")\n",
    "        chat_file = gr.File(label=\"Optional: Upload Medical Report\")\n",
    "        chat_output = gr.Textbox(label=\"AI's Response\")\n",
    "        chat_btn = gr.Button(\"Ask\")\n",
    "        chat_btn.click(chat_with_llm, inputs=[chat_input, chat_file], outputs=chat_output)\n",
    "\n",
    "    with gr.Tab(\"🩺 Find a Doctor\"):\n",
    "        symptoms_input = gr.Textbox(lines=3, placeholder=\"Describe your symptoms...\", label=\"Symptoms\")\n",
    "        doctor_output = gr.Textbox(label=\"Recommended Doctors\")\n",
    "        doctor_btn = gr.Button(\"Find Doctors\")\n",
    "        doctor_btn.click(recommend_doctors, inputs=symptoms_input, outputs=doctor_output)\n",
    "\n",
    "\n",
    "interface.launch(share=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
